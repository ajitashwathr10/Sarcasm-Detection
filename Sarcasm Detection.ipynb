{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c09d049-64dd-4d0d-8fc0-ea017daa4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f54a28f-7be0-49bb-82f8-327ef00a39c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a6d07f51434f7d804b33b4ecb7f860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa0336046f7454d930fc76e66524386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6711a9b469ac45a6b30f4222cdf33e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9832184f2445e4b5e3336da6fa6d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c3460af37d4d5f8750070a2ffc8622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37796c6915f34bdf9b3da2f302d7873e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd4c714d1d7402ebdef230a5d3c71e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "BertModel.from_pretrained('bert-base-uncased', force_download = True)\n",
    "BertTokenizer.from_pretrained('bert-base-uncased', force_download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b547c43-d936-47bf-aa2f-bcfc7435856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\AJIT ASHWATH\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\AJIT ASHWATH\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\AJIT ASHWATH\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AJIT ASHWATH R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "68572999-3f27-49f6-b054-12c42d060b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    def preprocess(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "        return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bfee3687-9c52-4674-bf9e-db2240cb61d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = 1 if self.labels[idx] == \"Sarcasm\" else 0\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens = True,\n",
    "            padding = \"max_length\",\n",
    "            truncation = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71053f22-9ea6-4937-9cf9-14d52baeec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSarcasmClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super(BERTSarcasmClassifier, self).__init__()\n",
    "        try:\n",
    "            self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading BERT model: {e}\")\n",
    "            print(\"Attempting to load with offline mode...\")\n",
    "            self.bert = BertModel.from_pretrained('bert-base-uncased', local_files_only=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        return self.linear(dropout_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3e869edf-685d-4826-9eea-7449449e7cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmDetector:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        try:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tokenizer: {e}\")\n",
    "            print(\"Attempting to load with offline mode...\")\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', local_files_only = True)\n",
    "        self.model = None\n",
    "        self.traditional_model = None\n",
    "\n",
    "    def train_bert_model(self, train_loader, val_loader, epochs = 3):\n",
    "        try:\n",
    "            self.model = BERTSarcasmClassifier().to(self.device)\n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr = 2e-5)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            for epoch in range(epochs):\n",
    "                self.model.train()\n",
    "                total_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                print(f\"\\nEpoch {epoch + 1} / {epochs}\")\n",
    "                print(\"Training progress:\")\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    labels = batch['label'].to(self.device)\n",
    "                    outputs = self.model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    if (batch_idx + 1) % 10 == 0:\n",
    "                        print(f\"Batch [{batch_idx + 1} / {len(train_loader)}] \"\n",
    "                              f\"Loss: {loss.item():.4f} \"\n",
    "                              f\"Accuracy: {100 * correct/total:.2f}%\")\n",
    "\n",
    "                val_accuracy = self.evaluate_bert(val_loader)\n",
    "                print(f'Epoch {epoch + 1} Summary:')\n",
    "                print(f'Average Loss: {total_loss / len(train_loader):.4f}')\n",
    "                print(f'Training Accuracy: {100 * correct / total:.2f}%')\n",
    "                print(f'Validation Accuracy: {100 * val_accuracy:.2f}%')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during BERT training: {e}\")\n",
    "            print(\"Falling back to traditional model only...\")\n",
    "            self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "10e123a5-f381-43ad-8b65-c1be290f1541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading and preparing data...\n",
      "An error occurred: 'SarcasmDetector' object has no attribute 'prepare_data'\n",
      "Please ensure all required libraries are installed and the data file path is correct.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        detector = SarcasmDetector()\n",
    "        print(\"Loading and preparing data...\")\n",
    "        data = detector.prepare_data(\"C:\\\\Users\\\\AJIT ASHWATH R\\\\Downloads\\\\Sarcasm.json\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data['processed_headline'],\n",
    "            data['is_sarcastic'],\n",
    "            test_size = 0.2,\n",
    "            random_state = 42\n",
    "        )\n",
    "        print(\"\\nTraining traditional model...\")\n",
    "        detector.train_traditional_model(X_train, y_train)\n",
    "        print(\"\\nPreparing BERT datasets...\")\n",
    "        train_dataset = SarcasmDataset(X_train.values, y_train.values, detector.tokenizer)\n",
    "        val_dataset = SarcasmDataset(X_test.values, y_test.values, detector.tokenizer)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = 32)\n",
    "        print(\"\\nTraining BERT model...\")\n",
    "        detector.train_bert_model(train_loader, val_loader)\n",
    "        test_texts = [\n",
    "            \"Scientists cure cancer with one simple trick\",\n",
    "            \"New study shows benefits of exercise\",\n",
    "            \"Area man becomes expert in everything after reading single article\"\n",
    "        ]\n",
    "        print(\"\\nTesting predictions:\")\n",
    "        for text in test_texts:\n",
    "            model_type = 'ensemble' if detector.model is not None else 'traditional'\n",
    "            prediction = detector.predict(text, model_type=model_type)\n",
    "            print(f\"\\nText: {text}\")\n",
    "            print(f\"Prediction: {prediction}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please ensure all required libraries are installed and the data file path is correct.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543aaec1-2e35-46a3-9f98-f896a837f23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ddb7f-90c2-4fc0-a221-66460d3393a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
